# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.18.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown] jp-MarkdownHeadingCollapsed=true
# # Äá»“ Ã¡n PhÃ¢n tÃ­ch & Dá»± Ä‘oÃ¡n GiÃ¡ nhÃ 
#
# **Má»¥c tiÃªu:**
# 1. PhÃ¢n tÃ­ch khÃ¡m phÃ¡ (EDA) bá»™ dá»¯ liá»‡u Ames Housing.
# 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u vÃ  xÃ¢y dá»±ng cÃ¡c pipeline xá»­ lÃ½.
# 3. So sÃ¡nh 6 mÃ´ hÃ¬nh Machine Learning:
#     - Linear Regression
#     - Ridge Regression
#     - Lasso Regression
#     - SVM (SVR)
#     - Random Forest Regressor
#     - XGBoost Regressor
# 4. TÃ¬m ra mÃ´ hÃ¬nh tá»‘t nháº¥t, tinh chá»‰nh vÃ  phÃ¢n tÃ­ch cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng.
# 5. LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t vÃ o file `.joblib` Ä‘á»ƒ triá»ƒn khai web app.

# %% [markdown]
# ## 1. ğŸ› ï¸ Khá»Ÿi táº¡o (Setup)
#
# Táº£i táº¥t cáº£ cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho dá»± Ã¡n:

# %%
# === ThÆ° viá»‡n Cá»‘t lÃµi (Core Libraries) ===
import pandas as pd
import numpy as np

# === Trá»±c quan hÃ³a (Visualization) ===
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats # DÃ¹ng cho Q-Q plot vÃ  kiá»ƒm tra Ä‘á»™ lá»‡ch (skewness)

# === Tiá»n xá»­ lÃ½ (Preprocessing) - scikit-learn ===
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# === MÃ´ hÃ¬nh (Models) - scikit-learn ===
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor

# === MÃ´ hÃ¬nh (Models) - BÃªn ngoÃ i ===
from xgboost import XGBRegressor # MÃ´ hÃ¬nh XGBoost

# === ÄÃ¡nh giÃ¡ (Evaluation) - scikit-learn ===
from sklearn.metrics import mean_squared_error, r2_score

# === Tiá»‡n Ã­ch (Utils) ===
import joblib # DÃ¹ng Ä‘á»ƒ lÆ°u vÃ  táº£i mÃ´ hÃ¬nh
import warnings # DÃ¹ng Ä‘á»ƒ táº¯t cÃ¡c cáº£nh bÃ¡o khÃ´ng cáº§n thiáº¿t

# === CÃ i Ä‘áº·t cho Notebook ===
# Äáº·t style chung cho cÃ¡c biá»ƒu Ä‘á»“
plt.style.use('ggplot')
# Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ ngay trong notebook
# %matplotlib inline
# Táº¯t cÃ¡c cáº£nh bÃ¡o (optional)
warnings.filterwarnings('ignore')

print("Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.")

# %% [markdown]
# ## 2. ğŸ’¾ Táº£i Dá»¯ liá»‡u (Load Data)
#
# Äá»c dá»¯ liá»‡u tá»« thÆ° má»¥c `data/`. ChÃºng ta sáº½ thá»±c hiá»‡n táº¥t cáº£ phÃ¢n tÃ­ch vÃ 
# huáº¥n luyá»‡n trÃªn `train.csv`.

# %%
# Äáº·t Ä‘Æ°á»ng dáº«n file
DATA_DIR = '../data/'
TRAIN_FILE = DATA_DIR + 'train.csv'
TEST_FILE = DATA_DIR + 'test.csv'
DATA_DESC_FILE = DATA_DIR + 'data_description.txt'

# Táº£i dá»¯ liá»‡u huáº¥n luyá»‡n
try:
    df_train = pd.read_csv(TRAIN_FILE)
    print("Táº£i file train.csv thÃ nh cÃ´ng.")
except FileNotFoundError:
    print(f"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file táº¡i {TRAIN_FILE}")
    print("HÃ£y Ä‘áº£m báº£o file train.csv náº±m trong thÆ° má»¥c data/")

# %%
# Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u tiÃªn
print(f"HÃ¬nh dáº¡ng cá»§a dá»¯ liá»‡u (HÃ ng, Cá»™t): {df_train.shape}")
df_train.head()

# %%
# Kiá»ƒm tra thÃ´ng tin cÃ¡c cá»™t (kiá»ƒu dá»¯ liá»‡u, sá»‘ lÆ°á»£ng non-null)
# ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng Ä‘á»ƒ chuáº©n bá»‹ cho viá»‡c xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u (Missing Values)
df_train.info()

# %% [markdown]
# ## 3. ğŸ“Š PhÃ¢n tÃ­ch KhÃ¡m phÃ¡ & Tiá»n xá»­ lÃ½ (EDA & Preprocessing)
#
# ### 3.1. PhÃ¢n tÃ­ch Biáº¿n má»¥c tiÃªu (`SalePrice`)
#
# Biáº¿n má»¥c tiÃªu cá»§a chÃºng ta lÃ  `SalePrice`. ChÃºng ta cáº§n kiá»ƒm tra phÃ¢n phá»‘i cá»§a nÃ³ Ä‘á»ƒ xem nÃ³ cÃ³ bá»‹ lá»‡ch (skewed) hay khÃ´ng.

# %%
# Äáº·t kÃ­ch thÆ°á»›c biá»ƒu Ä‘á»“
plt.figure(figsize=(14, 6))

# Váº½ biá»ƒu Ä‘á»“ Histogram
plt.subplot(1, 2, 1)
sns.histplot(df_train['SalePrice'], kde=True, bins=50)
plt.title('PhÃ¢n phá»‘i gá»‘c cá»§a SalePrice')
plt.xlabel('GiÃ¡ bÃ¡n (Sale Price)')
plt.ylabel('Táº§n suáº¥t (Frequency)')

# %% [markdown]
#
#
# > **Nháº­n xÃ©t (tá»« Histogram):** Biá»ƒu Ä‘á»“ rÃµ rÃ ng bá»‹ **lá»‡ch pháº£i
# (right-skewed)**. Háº§u háº¿t cÃ¡c nhÃ  cÃ³ giÃ¡ trá»‹ tháº¥p Ä‘áº¿n trung bÃ¬nh, vÃ  cÃ³ má»™t
# "Ä‘uÃ´i" dÃ i gá»“m cÃ¡c cÄƒn nhÃ  ráº¥t Ä‘áº¯t (giÃ¡ trá»‹ ngoáº¡i lá»‡).

# %%
# TÃ­nh toÃ¡n Ä‘á»™ lá»‡ch (Skewness)
skewness = df_train['SalePrice'].skew()
print(f"Äá»™ lá»‡ch (Skewness) cá»§a SalePrice: {skewness:.4f}")

# Váº½ biá»ƒu Ä‘á»“ Q-Q (Quantile-Quantile)
plt.subplot(1, 2, 2)
stats.probplot(df_train['SalePrice'], plot=plt)
plt.title('Biá»ƒu Ä‘á»“ Q-Q cá»§a SalePrice')

plt.tight_layout()
plt.show()

# %% [markdown]
#
#
# > **Nháº­n xÃ©t (tá»« Skewness & Q-Q Plot):**
# > * **Äá»™ lá»‡ch (Skewness):** GiÃ¡ trá»‹ ~1.88 (lá»›n hÆ¡n 1) xÃ¡c nháº­n má»©c Ä‘á»™ lá»‡ch pháº£i lÃ  Ä‘Ã¡ng ká»ƒ.
# > * **Biá»ƒu Ä‘á»“ Q-Q:** CÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u (mÃ u xanh) khÃ´ng náº±m trÃªn Ä‘Æ°á»ng chÃ©o mÃ u Ä‘á». ChÃºng bá»‹ uá»‘n cong á»Ÿ pháº§n Ä‘uÃ´i trÃªn, xÃ¡c nháº­n ráº±ng dá»¯ liá»‡u khÃ´ng tuÃ¢n theo phÃ¢n phá»‘i chuáº©n.

# %% [markdown]
# ### Giáº£i phÃ¡p: Biáº¿n Ä‘á»•i Logarit (Log Transformation)
#
# Äá»ƒ kháº¯c phá»¥c Ä‘á»™ lá»‡ch nÃ y, chÃºng ta sáº½ Ã¡p dá»¥ng phÃ©p biáº¿n Ä‘á»•i logarit. ChÃºng ta sáº½ sá»­ dá»¥ng `np.log1p` (tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i `log(1 + x)`) Ä‘á»ƒ xá»­ lÃ½ an toÃ n má»i giÃ¡ trá»‹ báº±ng 0 (máº·c dÃ¹ á»Ÿ Ä‘Ã¢y giÃ¡ nhÃ  khÃ´ng báº±ng 0).
#
# ChÃºng ta sáº½ táº¡o má»™t cá»™t má»›i `SalePrice_Log` vÃ  dÃ¹ng nÃ³ lÃ m biáº¿n má»¥c tiÃªu má»›i.

# %%
# Ãp dá»¥ng phÃ©p biáº¿n Ä‘á»•i log1p
df_train['SalePrice_Log'] = np.log1p(df_train['SalePrice'])

# Váº½ láº¡i biá»ƒu Ä‘á»“ cho cá»™t má»›i
plt.figure(figsize=(14, 6))

# Biá»ƒu Ä‘á»“ Histogram má»›i
plt.subplot(1, 2, 1)
sns.histplot(df_train['SalePrice_Log'], kde=True, bins=50, color='blue')
plt.title('PhÃ¢n phá»‘i cá»§a SalePrice (Sau khi biáº¿n Ä‘á»•i Log)')
plt.xlabel('Log(1 + Sale Price)')
plt.ylabel('Táº§n suáº¥t (Frequency)')

# Biá»ƒu Ä‘á»“ Q-Q má»›i
plt.subplot(1, 2, 2)
stats.probplot(df_train['SalePrice_Log'], plot=plt)
plt.title('Biá»ƒu Ä‘á»“ Q-Q cá»§a SalePrice (Sau khi biáº¿n Ä‘á»•i Log)')

plt.tight_layout()
plt.show()

# %%
# Kiá»ƒm tra láº¡i Ä‘á»™ lá»‡ch má»›i
new_skewness = df_train['SalePrice_Log'].skew()
print(f"Äá»™ lá»‡ch (Skewness) Má»šI cá»§a SalePrice_Log: {new_skewness:.4f}")

# %% [markdown]
# > **Káº¿t luáº­n:** ThÃ nh cÃ´ng!
# > * Äá»™ lá»‡ch má»›i bÃ¢y giá» ráº¥t gáº§n 0 (~0.12).
# > * Biá»ƒu Ä‘á»“ phÃ¢n phá»‘i trÃ´ng gáº§n nhÆ° hÃ¬nh chuÃ´ng (phÃ¢n phá»‘i chuáº©n).
# > * Biá»ƒu Ä‘á»“ Q-Q cho tháº¥y cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u bÃ¢y giá» náº±m gáº§n nhÆ° tháº³ng hÃ ng trÃªn Ä‘Æ°á»ng chÃ©o mÃ u Ä‘á».
# >
# > **Tá»« giá» trá»Ÿ Ä‘i, chÃºng ta sáº½ huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n `SalePrice_Log`.**

# %% [markdown]
# ### 3.2. PhÃ¢n tÃ­ch TÆ°Æ¡ng quan Äáº·c trÆ°ng (Feature Correlation)
#
# BÃ¢y giá», chÃºng ta sáº½ xem cÃ¡c Ä‘áº·c trÆ°ng sá»‘ (numerical features) nÃ o tÆ°Æ¡ng quan
# máº¡nh nháº¥t vá»›i `SalePrice`. Äiá»u nÃ y giÃºp chÃºng ta cÃ³ cÃ¡i nhÃ¬n ban Ä‘áº§u vá»
# nhá»¯ng yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n giÃ¡ nhiá»u nháº¥t.
#
# ChÃºng ta sáº½ tÃ­nh tÆ°Æ¡ng quan vá»›i `SalePrice` gá»‘c thay vÃ¬ `SalePrice_Log` á»Ÿ
# bÆ°á»›c nÃ y, vÃ¬ nÃ³ trá»±c quan hÆ¡n Ä‘á»ƒ hiá»ƒu má»‘i quan há»‡ ban Ä‘áº§u.

# %%
# TÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng quan (chá»‰ cho cÃ¡c cá»™t sá»‘)
corr_matrix = df_train.corr(numeric_only=True)

# Láº¥y 10 Ä‘áº·c trÆ°ng (features) cÃ³ tÆ°Æ¡ng quan máº¡nh nháº¥t vá»›i 'SalePrice'
k = 10
# Láº¥y ra 'k' hÃ ng Ä‘áº§u tiÃªn tá»« series Ä‘Ã£ sáº¯p xáº¿p giáº£m dáº§n
top_k_features = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index

print(f"Top {k} Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan máº¡nh nháº¥t vá»›i SalePrice:")
print(top_k_features)

# %% [markdown]
# BÃ¢y giá», chÃºng ta sáº½ váº½ má»™t **báº£n Ä‘á»“ nhiá»‡t (heatmap)** chá»‰ cho 10 Ä‘áº·c trÆ°ng nÃ y Ä‘á»ƒ xem má»‘i quan há»‡ *giá»¯a chÃºng* vÃ  *vá»›i SalePrice*.

# %%
# Láº¥y ma tráº­n tÆ°Æ¡ng quan con (subset) chá»‰ cho 10 Ä‘áº·c trÆ°ng hÃ ng Ä‘áº§u
top_k_corr_matrix = df_train[top_k_features].corr(numeric_only=True)

# Váº½ heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(top_k_corr_matrix,
            annot=True,        # Hiá»ƒn thá»‹ sá»‘ (giÃ¡ trá»‹ tÆ°Æ¡ng quan)
            cmap='viridis',    # Báº£ng mÃ u
            fmt='.2f')         # Äá»‹nh dáº¡ng sá»‘ (2 chá»¯ sá»‘ tháº­p phÃ¢n)

plt.title(f'Báº£n Ä‘á»“ nhiá»‡t TÆ°Æ¡ng quan cá»§a Top {k} Äáº·c trÆ°ng')
plt.show()

# %% [markdown]
#
#
# > **Nháº­n xÃ©t (tá»« Heatmap):**
# > * **TÆ°Æ¡ng quan vá»›i `SalePrice`:** `OverallQual` (Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ) cÃ³ tÆ°Æ¡ng quan máº¡nh nháº¥t (0.79), theo sau lÃ  `GrLivArea` (Diá»‡n tÃ­ch sá»‘ng) (0.71).
# > * **Äa cá»™ng tuyáº¿n (Multicollinearity):** ChÃºng ta tháº¥y má»™t sá»‘ Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan ráº¥t máº¡nh vá»›i nhau. ÄÃ¢y lÃ  má»™t váº¥n Ä‘á» tiá»m áº©n gá»i lÃ  "Ä‘a cá»™ng tuyáº¿n".
# >     * `GarageCars` vÃ  `GarageArea` (0.88): Ráº¥t logic (gara lá»›n hÆ¡n chá»©a Ä‘Æ°á»£c nhiá»u xe hÆ¡n).
# >     * `TotalBsmtSF` vÃ  `1stFlrSF` (0.82): Logic (tá»•ng diá»‡n tÃ­ch táº§ng háº§m thÆ°á»ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng diá»‡n tÃ­ch táº§ng 1).
# >     * `GrLivArea` vÃ  `TotRmsAbvGrd` (0.83): Logic (diá»‡n tÃ­ch lá»›n hÆ¡n thÃ¬ cÃ³ nhiá»u phÃ²ng hÆ¡n).
# >
# > CÃ¡c mÃ´ hÃ¬nh nhÆ° Ridge vÃ  Lasso (mÃ  chÃºng ta sáº½ cháº¡y) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ tá»‘t váº¥n Ä‘á» Ä‘a cá»™ng tuyáº¿n nÃ y.

# %% [markdown]
# ### 3.3. Xá»­ lÃ½ GiÃ¡ trá»‹ bá»‹ thiáº¿u (Missing Values)
#
# ChÃºng ta sáº½ khÃ´ng xá»­ lÃ½ thá»§ cÃ´ng cÃ¡c giÃ¡ trá»‹ thiáº¿u trÃªn DataFrame. Thay vÃ o
# Ä‘Ã³, chÃºng ta sáº½ Ä‘á»‹nh nghÄ©a **chiáº¿n lÆ°á»£c** vÃ  Ä‘Æ°a cÃ¡c chiáº¿n lÆ°á»£c nÃ y vÃ o
# `Pipeline` á»Ÿ Pháº§n 4. Äiá»u nÃ y giÃºp ngÄƒn ngá»«a rÃ² rá»‰ dá»¯ liá»‡u (data leakage).
#
# ChÃºng ta sáº½ kiá»ƒm tra nhanh cÃ¡c cá»™t bá»‹ thiáº¿u nhiá»u nháº¥t.

# %%
# TÃ­nh toÃ¡n % giÃ¡ trá»‹ bá»‹ thiáº¿u cho má»—i cá»™t
percent_missing = (df_train.isnull().sum() / len(df_train)) * 100
percent_missing = percent_missing[percent_missing > 0].sort_values(ascending=False)

# Hiá»ƒn thá»‹ 20 cá»™t cÃ³ % thiáº¿u cao nháº¥t
print(percent_missing.head(20))

# %% [markdown]
# > **Chiáº¿n lÆ°á»£c Tiá»n xá»­ lÃ½ (Cho Pipeline):**
# >
# > Dá»±a trÃªn `data_description.txt` vÃ  danh sÃ¡ch trÃªn:
# >
# > 1.  **CÃ¡c cá»™t Sá»‘ (Numerical):** Äá»‘i vá»›i cÃ¡c cá»™t nhÆ° `LotFrontage` (bá»‹ thiáº¿u tháº­t), chÃºng ta sáº½ dÃ¹ng `SimpleImputer` Ä‘á»ƒ Ä‘iá»n báº±ng **`median`** (trung vá»‹).
# > 2.  **CÃ¡c cá»™t PhÃ¢n loáº¡i (Categorical):**
# >     * Äá»‘i vá»›i cÃ¡c cá»™t mÃ  `NaN` cÃ³ Ã½ nghÄ©a (vÃ­ dá»¥: `PoolQC`, `Alley`, `Fence`, `FireplaceQu`, `GarageType`...): `NaN` cÃ³ nghÄ©a lÃ  "KhÃ´ng cÃ³ Há»“ bÆ¡i", "KhÃ´ng cÃ³ Háº»m"... ChÃºng ta sáº½ Ä‘iá»n báº±ng má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh lÃ  **`"None"`**.
# >     * Äá»‘i vá»›i cÃ¡c cá»™t mÃ  `NaN` lÃ  thiáº¿u tháº­t (vÃ­ dá»¥: `Electrical`): ChÃºng ta sáº½ Ä‘iá»n báº±ng **`most_frequent`** (giÃ¡ trá»‹ xuáº¥t hiá»‡n nhiá»u nháº¥t).
# >
# > (Trong Pháº§n 4.2, chÃºng ta sáº½ xÃ¢y dá»±ng cÃ¡c `Pipeline` riÃªng biá»‡t Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a viá»‡c nÃ y).

# %% [markdown]
# ### 3.4. Ká»¹ thuáº­t Äáº·c trÆ°ng (Feature Engineering)
#
# TÆ°Æ¡ng tá»± nhÆ° trÃªn, chÃºng ta sáº½ khÃ´ng táº¡o cÃ¡c Ä‘áº·c trÆ°ng má»›i thá»§ cÃ´ng. ChÃºng ta
# sáº½ Ä‘á»‹nh nghÄ©a **chiáº¿n lÆ°á»£c** vÃ  Ä‘Æ°a chÃºng vÃ o `Pipeline` (máº·c dÃ¹ Ä‘á»ƒ Ä‘Æ¡n giáº£n,
# chÃºng ta cÃ³ thá»ƒ táº¡o chÃºng trÆ°á»›c khi chia dá»¯ liá»‡u náº¿u chÃºng khÃ´ng gÃ¢y rÃ² rá»‰).
#
# Tuy nhiÃªn, Ä‘á»ƒ giá»¯ cho `Pipeline` Ä‘Æ¡n giáº£n nháº¥t, chÃºng ta sáº½ chá»‰ táº­p trung vÃ o
# cÃ¡c Ä‘áº·c trÆ°ng gá»‘c trong bÆ°á»›c so sÃ¡nh mÃ´ hÃ¬nh.
#
# **VÃ¬ má»¥c tiÃªu cá»§a Ä‘á»“ Ã¡n nÃ y lÃ  so sÃ¡nh 6 mÃ´ hÃ¬nh, chÃºng ta sáº½ sá»­ dá»¥ng cÃ¡c Ä‘áº·c
# trÆ°ng gá»‘c (raw features) Ä‘á»ƒ giá»¯ cho `Pipeline` tiá»n xá»­ lÃ½ nháº¥t quÃ¡n vÃ  cÃ´ng
# báº±ng cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh.**

# %% [markdown]
# ## 4. âš™ï¸ XÃ¢y dá»±ng Pipeline & Huáº¥n luyá»‡n MÃ´ hÃ¬nh

# %% [markdown]
# ### 4.1. PhÃ¢n chia Dá»¯ liá»‡u (Train/Test Split)
#
# Äáº§u tiÃªn, chÃºng ta sáº½ tÃ¡ch `train.csv` (1460 hÃ ng) thÃ nh 2 pháº§n:
# 1.  **Táº­p Huáº¥n luyá»‡n (Training set):** 80% dá»¯ liá»‡u (1168 hÃ ng) Ä‘á»ƒ "dáº¡y" mÃ´ hÃ¬nh.
# 2.  **Táº­p Kiá»ƒm tra (Validation set):** 20% dá»¯ liá»‡u (292 hÃ ng) Ä‘á»ƒ "kiá»ƒm tra" hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u láº¡.
#
# ChÃºng ta pháº£i lÃ m Ä‘iá»u nÃ y *trÆ°á»›c khi* thá»±c hiá»‡n báº¥t ká»³ bÆ°á»›c tiá»n xá»­ lÃ½ nÃ o (nhÆ° `Imputation` hay `Scaling`) Ä‘á»ƒ ngÄƒn cháº·n RÃ² rá»‰ Dá»¯ liá»‡u (Data Leakage).

# %%
# 1. Chá»n Äáº·c trÆ°ng (X) vÃ  Biáº¿n má»¥c tiÃªu (y)
#
# Biáº¿n má»¥c tiÃªu (y) cá»§a chÃºng ta lÃ  'SalePrice_Log'
y = df_train['SalePrice_Log']

# Äáº·c trÆ°ng (X) lÃ  táº¥t cáº£ cÃ¡c cá»™t cÃ²n láº¡i,
# ngoáº¡i trá»« 'Id', 'SalePrice' gá»‘c, vÃ  'SalePrice_Log'
X = df_train.drop(['Id', 'SalePrice', 'SalePrice_Log'], axis=1)

# %%
# 2. PhÃ¢n chia Dá»¯ liá»‡u
#
# ChÃºng ta dÃ¹ng test_size=0.2 Ä‘á»ƒ dÃ nh 20% cho táº­p kiá»ƒm tra (validation).
# random_state=42 lÃ  má»™t con sá»‘ báº¥t ká»³, nhÆ°ng nÃ³ Ä‘áº£m báº£o ráº±ng
# má»—i láº§n báº¡n cháº¡y láº¡i code, dá»¯ liá»‡u luÃ´n Ä‘Æ°á»£c chia theo CÃ™NG Má»˜T CÃCH.
# Äiá»u nÃ y giÃºp káº¿t quáº£ cá»§a báº¡n cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¡i táº¡o (reproducible).
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# %%
# 3. Kiá»ƒm tra káº¿t quáº£
print("--- KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau khi chia ---")
print(f"KÃ­ch thÆ°á»›c X_train (Huáº¥n luyá»‡n): {X_train.shape}")
print(f"KÃ­ch thÆ°á»›c y_train (Huáº¥n luyá»‡n): {y_train.shape}")
print(f"KÃ­ch thÆ°á»›c X_val (Kiá»ƒm tra):    {X_val.shape}")
print(f"KÃ­ch thÆ°á»›c y_val (Kiá»ƒm tra):    {y_val.shape}")

# %% [markdown]
# ### 4.2. Äá»‹nh nghÄ©a Pipeline Tiá»n xá»­ lÃ½
#
# Thay vÃ¬ xá»­ lÃ½ thá»§ cÃ´ng, chÃºng ta sáº½ táº¡o má»™t `Pipeline` tiá»n xá»­ lÃ½ duy nháº¥t. `Pipeline` nÃ y sáº½:
# 1.  XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t sá»‘ (numerical) vÃ  cá»™t phÃ¢n loáº¡i (categorical).
# 2.  Ãp dá»¥ng cÃ¡c phÃ©p biáº¿n Ä‘á»•i khÃ¡c nhau cho tá»«ng nhÃ³m cá»™t.
# 3.  Äáº£m báº£o ráº±ng cÃ¡c phÃ©p biáº¿n Ä‘á»•i (nhÆ° tÃ­nh `median` hoáº·c `scaling`) chá»‰ Ä‘Æ°á»£c "há»c" tá»« `X_train`.

# %%
# 1. Tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh cÃ¡c loáº¡i cá»™t Dá»°A TRÃŠN X_train
# Äiá»u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta khÃ´ng vÃ´ tÃ¬nh
# Ä‘Æ°a thÃ´ng tin tá»« X_val vÃ o.
numerical_features = X_train.select_dtypes(include=np.number).columns
categorical_features = X_train.select_dtypes(include=object).columns

print(f"Sá»‘ Ä‘áº·c trÆ°ng sá»‘ (numerical): {len(numerical_features)}")
print(f"CÃ¡c cá»™t sá»‘ (vÃ­ dá»¥): {list(numerical_features[:5])}")
print(f"Sá»‘ Ä‘áº·c trÆ°ng phÃ¢n loáº¡i (categorical): {len(categorical_features)}")
print(f"CÃ¡c cá»™t phÃ¢n loáº¡i (vÃ­ dá»¥): {list(categorical_features[:5])}")

# %%
# 2. Táº¡o pipeline cho Ä‘áº·c trÆ°ng Sá» (Numerical Transformer)
#
# ChÃºng ta sáº½ thá»±c hiá»‡n 2 bÆ°á»›c:
# - SimpleImputer: Äiá»n cÃ¡c giÃ¡ trá»‹ `NaN` báº±ng giÃ¡ trá»‹ trung vá»‹ (`median`) cá»§a cá»™t Ä‘Ã³.
# - StandardScaler: Co giÃ£n (scale) táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ vá» cÃ¹ng má»™t thang Ä‘o (trung bÃ¬nh 0, Ä‘á»™ lá»‡ch chuáº©n 1).
#   (Ráº¥t quan trá»ng cho Linear Regression, Ridge, Lasso, vÃ  SVR).

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# %%
# 3. Táº¡o pipeline cho Ä‘áº·c trÆ°ng PHÃ‚N LOáº I (Categorical Transformer)
#
# ChÃºng ta sáº½ thá»±c hiá»‡n 2 bÆ°á»›c:
# - SimpleImputer: Äiá»n cÃ¡c giÃ¡ trá»‹ `NaN` báº±ng giÃ¡ trá»‹ phá»• biáº¿n nháº¥t (`most_frequent`) cá»§a cá»™t Ä‘Ã³.
#   (ÄÃ¢y lÃ  chiáº¿n lÆ°á»£c cÆ¡ sá»Ÿ an toÃ n mÃ  chÃºng ta Ä‘Ã£ tháº£o luáº­n).
# - OneHotEncoder: Chuyá»ƒn Ä‘á»•i cÃ¡c giÃ¡ trá»‹ vÄƒn báº£n (vÃ­ dá»¥: 'RL', 'Pave') thÃ nh cÃ¡c cá»™t 0/1.
#   (handle_unknown='ignore' Ä‘áº£m báº£o mÃ´ hÃ¬nh khÃ´ng bá»‹ lá»—i náº¿u gáº·p má»™t danh má»¥c láº¡ á»Ÿ táº­p validation).

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# %%
# 4. Káº¿t há»£p hai pipeline láº¡i báº±ng ColumnTransformer
#
# ColumnTransformer sáº½ Ã¡p dá»¥ng `numeric_transformer` cho táº¥t cáº£ cÃ¡c cá»™t sá»‘
# vÃ  `categorical_transformer` cho táº¥t cáº£ cÃ¡c cá»™t phÃ¢n loáº¡i.
#

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Giá»¯ láº¡i cÃ¡c cá»™t khÃ´ng Ä‘Æ°á»£c liá»‡t kÃª (máº·c dÃ¹ á»Ÿ Ä‘Ã¢y chÃºng ta Ä‘Ã£ dÃ¹ng háº¿t)
)

print("ÄÃ£ táº¡o preprocessor (ColumnTransformer) thÃ nh cÃ´ng.")

# %% [markdown]
# ### 4.3. So sÃ¡nh 6 MÃ´ hÃ¬nh
#
# ÄÃ¢y lÃ  pháº§n chÃ­nh cá»§a thá»­ nghiá»‡m. ChÃºng ta sáº½ táº¡o má»™t vÃ²ng láº·p:
# 1. Äá»‹nh nghÄ©a 6 mÃ´ hÃ¬nh chÃºng ta muá»‘n so sÃ¡nh.
# 2. Vá»›i má»—i mÃ´ hÃ¬nh, táº¡o má»™t `Pipeline` hoÃ n chá»‰nh (káº¿t há»£p `preprocessor` vÃ  `model`).
# 3. Huáº¥n luyá»‡n (`fit`) pipeline trÃªn `X_train`.
# 4. Dá»± Ä‘oÃ¡n (`predict`) trÃªn `X_val`.
# 5. Äáº£o ngÆ°á»£c (inverse transform) cÃ¡c dá»± Ä‘oÃ¡n (`np.expm1`) Ä‘á»ƒ quay vá» giÃ¡ trá»‹ Ä‘Ã´ la ($).
# 6. TÃ­nh toÃ¡n RMSE vÃ  RÂ² trÃªn giÃ¡ trá»‹ Ä‘Ã´ la.
# 7. LÆ°u káº¿t quáº£.

# %%
# 1. Äá»‹nh nghÄ©a cÃ¡c mÃ´ hÃ¬nh
# (ChÃºng ta sá»­ dá»¥ng random_state=42 Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ nháº¥t quÃ¡n má»—i khi cháº¡y)
models = {
    "Linear Regression": LinearRegression(),
    "Ridge": Ridge(random_state=42),
    "Lasso": Lasso(random_state=42),
    "SVR": SVR(), # Support Vector Regressor
    "Random Forest": RandomForestRegressor(random_state=42, n_jobs=-1), # n_jobs=-1 Ä‘á»ƒ dÃ¹ng táº¥t cáº£ CPU
    "XGBoost": XGBRegressor(random_state=42, n_jobs=-1)
}

# 2. Chuáº©n bá»‹ list Ä‘á»ƒ lÆ°u káº¿t quáº£
results = []
pipelines = {} # LÆ°u láº¡i pipeline Ä‘á»ƒ dÃ¹ng sau (optional)

print("Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  so sÃ¡nh 6 mÃ´ hÃ¬nh...")

# 3. VÃ²ng láº·p qua cÃ¡c mÃ´ hÃ¬nh
for name, model in models.items():

    # 4. Táº¡o pipeline hoÃ n chá»‰nh
    # ÄÃ¢y lÃ  "dÃ¢y chuyá»n" cuá»‘i cÃ¹ng: Tiá»n xá»­ lÃ½ -> Huáº¥n luyá»‡n
    full_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor), # Tá»« Pháº§n 4.2
        ('model', model)
    ])

    # 5. Huáº¥n luyá»‡n (fit) pipeline trÃªn X_train
    print(f"--- Äang huáº¥n luyá»‡n: {name} ---")
    full_pipeline.fit(X_train, y_train)

    # 6. Dá»± Ä‘oÃ¡n trÃªn táº­p validation (X_val)
    # Káº¿t quáº£ dá»± Ä‘oÃ¡n váº«n á»Ÿ dáº¡ng logarit (y_pred_log)
    y_pred_log = full_pipeline.predict(X_val)

    # 7. QUAN TRá»ŒNG: Äáº£o ngÆ°á»£c (inverse transform) vá» giÃ¡ trá»‹ $
    # ChÃºng ta pháº£i so sÃ¡nh trÃªn cÃ¹ng Ä‘Æ¡n vá»‹ (Ä‘Ã´ la)
    y_val_dollar = np.expm1(y_val)
    y_pred_dollar = np.expm1(y_pred_log)

    # 8. TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
    # RMSE (Root Mean Squared Error) - Sai sá»‘ trung bÃ¬nh (tÃ­nh báº±ng $)
    rmse = np.sqrt(mean_squared_error(y_val_dollar, y_pred_dollar))

    # R-squared (RÂ²) - Má»©c Ä‘á»™ mÃ´ hÃ¬nh giáº£i thÃ­ch Ä‘Æ°á»£c dá»¯ liá»‡u
    r2 = r2_score(y_val_dollar, y_pred_dollar)

    # 9. LÆ°u káº¿t quáº£
    results.append({
        "Model": name,
        "RMSE ($)": rmse,
        "R-squared": r2
    })

    # LÆ°u láº¡i pipeline Ä‘Ã£ huáº¥n luyá»‡n (optional)
    pipelines[name] = full_pipeline

    print(f"HoÃ n thÃ nh: {name} | RMSE: ${rmse:,.2f} | RÂ²: {r2:.4f}")

print("\nÄÃ£ huáº¥n luyá»‡n xong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh.")

# %% [markdown]
# ### 4.4. Hiá»ƒn thá»‹ Báº£ng so sÃ¡nh Káº¿t quáº£
#
# Chuyá»ƒn list `results` (tá»« Pháº§n 4.3) thÃ nh má»™t DataFrame cá»§a Pandas Ä‘á»ƒ dá»… dÃ ng xem vÃ  so sÃ¡nh. ChÃºng ta sáº½ sáº¯p xáº¿p theo RMSE Ä‘á»ƒ xem mÃ´ hÃ¬nh nÃ o hoáº¡t Ä‘á»™ng tá»‘t nháº¥t (RMSE tháº¥p nháº¥t).

# %%
# Chuyá»ƒn list káº¿t quáº£ thÃ nh DataFrame
# (Biáº¿n 'results' Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  Ä‘iá»n á»Ÿ Ã´ 4.3)
results_df = pd.DataFrame(results)

# Sáº¯p xáº¿p DataFrame theo 'RMSE ($)' tá»« tháº¥p Ä‘áº¿n cao
results_df = results_df.sort_values(by='RMSE ($)', ascending=True)

# Hiá»ƒn thá»‹ báº£ng káº¿t quáº£
print("Báº£ng So sÃ¡nh Hiá»‡u suáº¥t 6 MÃ´ hÃ¬nh:")
# .style.format() dÃ¹ng Ä‘á»ƒ Ä‘á»‹nh dáº¡ng sá»‘ cho dá»… Ä‘á»c
results_df.style.format({
    "RMSE ($)": "${:,.2f}",
    "R-squared": "{:.4f}"
})

# %% [markdown]
# > **Nháº­n xÃ©t (Dá»±a trÃªn Báº£ng):**
# > 
# > 1.  **MÃ´ hÃ¬nh Tá»‘t nháº¥t:** `LinearRegression` (Há»“i quy Tuyáº¿n tÃ­nh) cho káº¿t quáº£ tá»‘t nháº¥t má»™t cÃ¡ch Ä‘Ã¡ng ngáº¡c nhiÃªn, vá»›i **RMSE lÃ  $22,741** vÃ  **RÂ² lÃ  0.9326**. Äiá»u nÃ y cho tháº¥y ráº±ng sau khi thá»±c hiá»‡n cÃ¡c bÆ°á»›c Tiá»n xá»­ lÃ½ (Ä‘áº·c biá»‡t lÃ  `log(SalePrice)`, `StandardScaler`, vÃ  `OneHotEncoder`), má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘áº·c trÆ°ng vÃ  biáº¿n má»¥c tiÃªu Ä‘Ã£ trá»Ÿ nÃªn ráº¥t tuyáº¿n tÃ­nh.
# > 
# > 2.  **MÃ´ hÃ¬nh Tá»‡ nháº¥t (Lasso):** MÃ´ hÃ¬nh `Lasso` Ä‘Ã£ tháº¥t báº¡i tháº£m háº¡i, cho káº¿t quáº£ RMSE khá»•ng lá»“ vÃ  RÂ² Ã¢m (cÃ³ nghÄ©a lÃ  nÃ³ dá»± Ä‘oÃ¡n cÃ²n tá»‡ hÆ¡n lÃ  chá»‰ Ä‘oÃ¡n giÃ¡ trá»‹ trung bÃ¬nh). Äiá»u nÃ y cÃ³ thá»ƒ lÃ  do `Lasso` (vá»›i tham sá»‘ `alpha` máº·c Ä‘á»‹nh) Ä‘Ã£ **Ä‘iá»u chuáº©n quÃ¡ má»©c** (over-regularized), nÃ³ Ä‘Ã£ "chá»n" Ä‘áº·c trÆ°ng quÃ¡ tay vÃ  cÃ³ thá»ƒ Ä‘Ã£ Ä‘áº·t há»‡ sá»‘ cá»§a háº§u háº¿t cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng vá» 0.
# > 
# > 3.  **CÃ¡c mÃ´ hÃ¬nh Ensemble:** `XGBoost` (RMSE \$26k) vÃ  `Random Forest` (RMSE \$29k) hoáº¡t Ä‘á»™ng ráº¥t tá»‘t, tá»‘t hÆ¡n nhiá»u so vá»›i `Lasso` vÃ  `SVR`, nhÆ°ng váº«n khÃ´ng thá»ƒ Ä‘Ã¡nh báº¡i mÃ´ hÃ¬nh `LinearRegression` Ä‘Æ¡n giáº£n. Äiá»u nÃ y cá»§ng cá»‘ giáº£ thuyáº¿t ráº±ng cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p nÃ y cÃ³ thá»ƒ Ä‘ang bá»‹ **overfitting** (há»c váº¹t) má»™t chÃºt vá»›i cÃ¡c tham sá»‘ máº·c Ä‘á»‹nh cá»§a chÃºng.


# %% [markdown]
# ## 5. ğŸ† Tinh chá»‰nh & PhÃ¢n tÃ­ch MÃ´ hÃ¬nh Tá»‘t nháº¥t
#
# Tá»« Pháº§n 4, `LinearRegression` lÃ  mÃ´ hÃ¬nh chiáº¿n tháº¯ng cá»§a chÃºng ta vá»›i RMSE tháº¥p nháº¥t.
#
# ### 5.1. PhÃ¢n tÃ­ch MÃ´ hÃ¬nh Tá»‘t nháº¥t (`LinearRegression`)
#
# CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn cÃ¢y (nhÆ° `XGBoost`) cÃ³ `feature_importance_`. CÃ¡c mÃ´
# hÃ¬nh tuyáº¿n tÃ­nh (nhÆ° `LinearRegression`) cÃ³ `coef_` (há»‡ sá»‘).
#
# Há»‡ sá»‘ cho chÃºng ta biáº¿t: "Khi má»™t Ä‘áº·c trÆ°ng tÄƒng 1 Ä‘Æ¡n vá»‹, `SalePrice_Log` sáº½
# tÄƒng (hoáº·c giáº£m) bao nhiÃªu", *giáº£ sá»­ táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c giá»¯ nguyÃªn*.
#
# ChÃºng ta sáº½ trÃ­ch xuáº¥t cÃ¡c há»‡ sá»‘ nÃ y tá»« `Pipeline` Ä‘Ã£ huáº¥n luyá»‡n.

# %%
# 1. Láº¥y ra pipeline cá»§a LinearRegression Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n
# (ChÃºng ta Ä‘Ã£ lÆ°u nÃ³ vÃ o dict 'pipelines' á»Ÿ Pháº§n 4.3)
lr_pipeline = pipelines['Linear Regression']

# 2. Láº¥y ra mÃ´ hÃ¬nh LinearRegression thá»±c sá»± tá»« bÃªn trong pipeline
lr_model = lr_pipeline.named_steps['model']

# 3. Láº¥y ra bá»™ tiá»n xá»­ lÃ½ (preprocessor) tá»« pipeline
preprocessor = lr_pipeline.named_steps['preprocessor']

# %% [markdown]
# BÆ°á»›c tiáº¿p theo hÆ¡i phá»©c táº¡p. ChÃºng ta cáº§n láº¥y tÃªn cá»§a táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng
# *sau khi* chÃºng Ä‘Ã£ Ä‘Æ°á»£c `OneHotEncoder` biáº¿n Ä‘á»•i (vÃ­ dá»¥: `MSZoning_RL`,
# `MSZoning_RM`...).

# %%
# 4. Láº¥y tÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c biáº¿n Ä‘á»•i
# Láº¥y tÃªn cÃ¡c cá»™t sá»‘ (Ä‘Ã£ Ä‘Æ°á»£c scale)
num_features = preprocessor.transformers_[0][2] # Láº¥y list tÃªn cá»™t sá»‘

# Láº¥y tÃªn cÃ¡c cá»™t phÃ¢n loáº¡i (Ä‘Ã£ Ä‘Æ°á»£c one-hot)
# ChÃºng ta truy cáº­p vÃ o 'onehot' step bÃªn trong 'cat' transformer
cat_features = list(preprocessor.named_transformers_['cat']
                    .named_steps['onehot']
                    .get_feature_names_out(categorical_features))

# Káº¿t há»£p cáº£ hai danh sÃ¡ch tÃªn
# ÄÃ¢y lÃ  danh sÃ¡ch Ä‘áº§y Ä‘á»§ cÃ¡c Ä‘áº·c trÆ°ng mÃ  mÃ´ hÃ¬nh thá»±c sá»± "nhÃ¬n tháº¥y"
all_feature_names = list(num_features) + cat_features

print(f"Tá»•ng sá»‘ Ä‘áº·c trÆ°ng mÃ  mÃ´ hÃ¬nh nhÃ¬n tháº¥y: {len(all_feature_names)}")

# %%
# 5. Táº¡o má»™t DataFrame Ä‘á»ƒ xem cÃ¡c há»‡ sá»‘ (coefficients)
# lr_model.coef_ lÃ  má»™t máº£ng NumPy chá»©a táº¥t cáº£ cÃ¡c há»‡ sá»‘
coefficients = pd.DataFrame({
    'Feature': all_feature_names,
    'Coefficient': lr_model.coef_
})

# Sáº¯p xáº¿p theo giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i Ä‘á»ƒ xem áº£nh hÆ°á»Ÿng lá»›n nháº¥t
coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()
coefficients = coefficients.sort_values(by='Abs_Coefficient', ascending=False)

# Hiá»ƒn thá»‹ 20 Ä‘áº·c trÆ°ng cÃ³ áº£nh hÆ°á»Ÿng (cáº£ Ã¢m vÃ  dÆ°Æ¡ng) lá»›n nháº¥t
print("Top 20 Äáº·c trÆ°ng áº£nh hÆ°á»Ÿng nháº¥t (Há»‡ sá»‘):")
coefficients.head(20)

# %% [markdown]
# ### Trá»±c quan hÃ³a cÃ¡c Há»‡ sá»‘

# %%
# Láº¥y 15 Ä‘áº·c trÆ°ng áº£nh hÆ°á»Ÿng TÄ‚NG GIÃ máº¡nh nháº¥t (há»‡ sá»‘ dÆ°Æ¡ng)
top_positive_coeffs = coefficients.sort_values(by='Coefficient', ascending=False).head(15)

# Láº¥y 15 Ä‘áº·c trÆ°ng áº£nh hÆ°á»Ÿng GIáº¢M GIÃ máº¡nh nháº¥t (há»‡ sá»‘ Ã¢m)
top_negative_coeffs = coefficients.sort_values(by='Coefficient', ascending=True).head(15)

# Káº¿t há»£p chÃºng láº¡i Ä‘á»ƒ váº½
top_coeffs_to_plot = pd.concat([top_positive_coeffs, top_negative_coeffs]).sort_values(by='Coefficient')

# Váº½ biá»ƒu Ä‘á»“ ngang
plt.figure(figsize=(12, 10))
plt.barh(top_coeffs_to_plot['Feature'], top_coeffs_to_plot['Coefficient'])
plt.title('áº¢nh hÆ°á»Ÿng cá»§a Top Äáº·c trÆ°ng Ä‘áº¿n log(SalePrice)')
plt.xlabel('Há»‡ sá»‘ (Coefficient)')
plt.ylabel('Äáº·c trÆ°ng')
plt.grid(axis='x')

# LÆ°u biá»ƒu Ä‘á»“ vÃ o thÆ° má»¥c bÃ¡o cÃ¡o
plt.savefig('../report/images/linear_regression_coefficients.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# > **Nháº­n xÃ©t (PhÃ¢n tÃ­ch Biá»ƒu Ä‘á»“ Há»‡ sá»‘):**
# >
# > Biá»ƒu Ä‘á»“ nÃ y cho chÃºng ta má»™t káº¿t quáº£ Ä‘Ã¡ng kinh ngáº¡c vÃ  lÃ  má»™t phÃ¡t hiá»‡n quan trá»ng cho Ä‘á»“ Ã¡n.
# >
# > 1.  **CÃ¡c Há»‡ sá»‘ "Bá»‹ Thá»•i phá»“ng" (Inflated Coefficients):**
# >     * Äáº·c trÆ°ng cÃ³ áº£nh hÆ°á»Ÿng **giáº£m giÃ¡** máº¡nh nháº¥t lÃ  `RoofMatl_ClyTile` (NgÃ³i Ä‘áº¥t sÃ©t).
# >     * Äáº·c trÆ°ng cÃ³ áº£nh hÆ°á»Ÿng **tÄƒng giÃ¡** máº¡nh nháº¥t lÃ  `RoofMatl_WdShngl` (VÃ¡n lá»£p gá»—).
# >     * CÃ¡c há»‡ sá»‘ nÃ y lá»›n má»™t cÃ¡ch phi lÃ½, láº¥n Ã¡t hoÃ n toÃ n cÃ¡c Ä‘áº·c trÆ°ng "logic" khÃ¡c.
# >
# > 2.  **Táº¡i sao Ä‘iá»u nÃ y xáº£y ra? (Dá»¯ liá»‡u Hiáº¿m/ThÆ°a thá»›t):**
# >     * ÄÃ¢y lÃ  má»™t triá»‡u chá»©ng kinh Ä‘iá»ƒn cá»§a viá»‡c mÃ´ hÃ¬nh tuyáº¿n tÃ­nh cá»‘ gáº¯ng "há»c váº¹t" tá»« cÃ¡c Ä‘áº·c trÆ°ng ráº¥t hiáº¿m. Trong 1460 cÄƒn nhÃ , cÃ³ thá»ƒ chá»‰ cÃ³ **1 hoáº·c 2** cÄƒn nhÃ  lá»£p báº±ng `ClyTile` (NgÃ³i Ä‘áº¥t sÃ©t), vÃ  chÃºng tÃ¬nh cá» cÃ³ giÃ¡ ráº¥t tháº¥p.
# >     * MÃ´ hÃ¬nh Ä‘Ã£ há»c má»™t quy táº¯c "ngu ngá»‘c": "Cá»© tháº¥y `ClyTile` lÃ  giÃ¡ cá»±c tháº¥p". Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u nÃ y, nÃ³ Ä‘Ã£ gÃ¡n má»™t há»‡ sá»‘ Ã¢m khá»•ng lá»“ cho Ä‘áº·c trÆ°ng Ä‘Ã³. Äiá»u tÆ°Æ¡ng tá»± xáº£y ra ngÆ°á»£c láº¡i vá»›i `WdShngl`.
# >
# > 3.  **CÃ¡c Äáº·c trÆ°ng "Biáº¿n máº¥t":**
# >     * ÄÃ¡ng chÃº Ã½, cÃ¡c Ä‘áº·c trÆ°ng mÃ  chÃºng ta biáº¿t lÃ  quan trá»ng tá»« PhÃ¢n tÃ­ch TÆ°Æ¡ng quan (Pháº§n 3.2) nhÆ° `GrLivArea`, `TotalBsmtSF`... hoÃ n toÃ n **khÃ´ng cÃ³ máº·t** trong top 30.
# >     * Äiá»u nÃ y ráº¥t cÃ³ thá»ƒ lÃ  do **Äa cá»™ng tuyáº¿n (Multicollinearity)**. Táº§m quan trá»ng cá»§a `GrLivArea` cÃ³ thá»ƒ Ä‘Ã£ bá»‹ "háº¥p thá»¥" bá»Ÿi cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c cÃ³ tÆ°Æ¡ng quan vá»›i nÃ³ (nhÆ° `OverallQual` hoáº·c cÃ¡c khu vá»±c `Neighborhood` "xá»‹n"), khiáº¿n há»‡ sá»‘ cá»§a `GrLivArea` bá»‹ Ä‘áº©y vá» gáº§n 0.
# >
# > **Káº¿t luáº­n quan trá»ng cho Äá»“ Ã¡n:**
# > Máº·c dÃ¹ `LinearRegression` cho chÃºng ta **Ä‘iá»ƒm dá»± Ä‘oÃ¡n (RMSE) tá»‘t nháº¥t**, nhÆ°ng nÃ³ lÃ  má»™t "há»™p Ä‘en" khÃ´ng Ä‘Ã¡ng tin cáº­y. CÃ¡c há»‡ sá»‘ cá»§a nÃ³ khÃ´ng á»•n Ä‘á»‹nh vÃ  bá»‹ áº£nh hÆ°á»Ÿng náº·ng ná» bá»Ÿi dá»¯ liá»‡u hiáº¿m vÃ  Ä‘a cá»™ng tuyáº¿n.
# >
# > Do Ä‘Ã³, chÃºng ta **khÃ´ng thá»ƒ** dÃ¹ng mÃ´ hÃ¬nh nÃ y Ä‘á»ƒ káº¿t luáº­n ráº±ng "Váº­t liá»‡u lá»£p mÃ¡i lÃ  yáº¿u tá»‘ quan trá»ng nháº¥t".
# >
# > Äiá»u nÃ y lÃ m cho viá»‡c phÃ¢n tÃ­ch cÃ¡c mÃ´ hÃ¬nh á»•n Ä‘á»‹nh hÆ¡n (nhÆ° `Ridge` vÃ  `XGBoost`) á»Ÿ bÆ°á»›c tiáº¿p theo cÃ ng trá»Ÿ nÃªn quan trá»ng.


# %% [markdown]
# ### 5.2. Tinh chá»‰nh (Tuning) cÃ¡c MÃ´ hÃ¬nh ThÃ¡ch thá»©c
#
# `LinearRegression` Ä‘Ã£ tháº¯ng, nhÆ°ng cÃ³ thá»ƒ cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n (nhÆ° `XGBoost`) Ä‘Ã£ bá»‹ "overfitting" vá»›i cÃ¡c cÃ i Ä‘áº·t máº·c Ä‘á»‹nh, hoáº·c cÃ¡c mÃ´ hÃ¬nh á»•n Ä‘á»‹nh hÆ¡n (nhÆ° `Ridge`) chÆ°a Ä‘Æ°á»£c tá»‘i Æ°u.
#
# ChÃºng ta sáº½ dÃ¹ng `GridSearchCV` Ä‘á»ƒ tÃ¬m ra cÃ¡c siÃªu tham sá»‘ (hyperparameters) tá»‘t nháº¥t cho `Ridge` vÃ  `XGBoost` Ä‘á»ƒ xem liá»‡u chÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh báº¡i má»‘c RMSE $22,741 hay khÃ´ng.
#
# #### 5.2.1. Tinh chá»‰nh `Ridge`
#
# SiÃªu tham sá»‘ quan trá»ng nháº¥t cá»§a `Ridge` lÃ  `alpha` - kiá»ƒm soÃ¡t "sá»©c máº¡nh" cá»§a Ä‘iá»u chuáº©n (regularization). ChÃºng ta sáº½ thá»­ má»™t loáº¡t cÃ¡c giÃ¡ trá»‹ `alpha` Ä‘á»ƒ tÃ¬m giÃ¡ trá»‹ tá»‘t nháº¥t.

# %%
print("--- Báº¯t Ä‘áº§u Tinh chá»‰nh Ridge ---")

# 1. Táº¡o má»™t pipeline Ridge Má»šI (chÆ°a huáº¥n luyá»‡n)
# (ChÃºng ta pháº£i dÃ¹ng má»™t pipeline má»›i Ä‘á»ƒ GridSearchCV cÃ³ thá»ƒ huáº¥n luyá»‡n nÃ³ nhiá»u láº§n)
pipe_ridge = Pipeline(steps=[
    ('preprocessor', preprocessor), # DÃ¹ng láº¡i preprocessor tá»« 4.2
    ('model', Ridge(random_state=42))
])

# 2. Äá»‹nh nghÄ©a lÆ°á»›i tham sá»‘ (Param Grid)
# ChÃºng ta sáº½ thá»­ 5 giÃ¡ trá»‹ alpha khÃ¡c nhau.
# 'model__alpha' -> CÃº phÃ¡p `__` (hai gáº¡ch dÆ°á»›i) lÃ  Ä‘á»ƒ nÃ³i
# "thay Ä‘á»•i tham sá»‘ 'alpha' bÃªn trong step tÃªn lÃ  'model'"
param_grid_ridge = {
    'model__alpha': [0.1, 1.0, 10.0, 50.0, 100.0]
}

# 3. Khá»Ÿi táº¡o GridSearchCV
# cv=5: Sá»­ dá»¥ng 5-fold Cross-Validation (chia X_train thÃ nh 5 pháº§n Ä‘á»ƒ kiá»ƒm tra chÃ©o)
# scoring='neg_root_mean_squared_error': ChÃºng ta muá»‘n tá»‘i Æ°u RMSE.
#     (Scikit-learn dÃ¹ng "neg" (Ã¢m) vÃ¬ nÃ³ luÃ´n cá»‘ gáº¯ng "tá»‘i Ä‘a hÃ³a" Ä‘iá»ƒm sá»‘,
#     vÃ  tá»‘i Ä‘a hÃ³a RMSE Ã¢m cÅ©ng chÃ­nh lÃ  tá»‘i thiá»ƒu hÃ³a RMSE).
grid_ridge = GridSearchCV(
    estimator=pipe_ridge,
    param_grid=param_grid_ridge,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1 # Sá»­ dá»¥ng táº¥t cáº£ cÃ¡c nhÃ¢n CPU Ä‘á»ƒ cháº¡y nhanh hÆ¡n
)

# 4. Huáº¥n luyá»‡n (Fit)
# BÆ°á»›c nÃ y sáº½ máº¥t má»™t chÃºt thá»i gian. NÃ³ Ä‘ang huáº¥n luyá»‡n 5 (alpha) * 5 (cv) = 25 mÃ´ hÃ¬nh.
grid_ridge.fit(X_train, y_train)

# 5. In káº¿t quáº£
print("\n--- Káº¿t quáº£ Tinh chá»‰nh Ridge ---")
print(f"Alpha tá»‘t nháº¥t: {grid_ridge.best_params_}")

# Láº¥y Ä‘iá»ƒm sá»‘ RMSE (á»Ÿ dáº¡ng log) tá»‘t nháº¥t
# ChÃºng ta pháº£i láº¥y trá»‹ tuyá»‡t Ä‘á»‘i (abs) vÃ¬ Ä‘iá»ƒm sá»‘ lÃ  Ã¢m
best_log_rmse_ridge = abs(grid_ridge.best_score_)
print(f"RMSE (trÃªn log scale) tá»‘t nháº¥t tá»« Cross-Validation: {best_log_rmse_ridge:.4f}")

# %% [markdown]
# > **Nháº­n xÃ©t (Ridge):**
# >
# > * **`Alpha tá»‘t nháº¥t: 50.0`**: ÄÃ¢y lÃ  má»™t phÃ¡t hiá»‡n cá»±c ká»³ quan trá»ng. `LinearRegression` cÆ¡ báº£n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i `Ridge` cÃ³ `alpha=0`. Viá»‡c `GridSearchCV` tÃ¬m ra giÃ¡ trá»‹ `alpha=50.0` (má»™t má»©c Ä‘iá»u chuáº©n khÃ¡ máº¡nh) lÃ  tá»‘t nháº¥t cho tháº¥y rÃµ ráº±ng:
# >     1.  MÃ´ hÃ¬nh `LinearRegression` gá»‘c (alpha=0) Ä‘Ã£ **khÃ´ng á»•n Ä‘á»‹nh**.
# >     2.  Viá»‡c thÃªm vÃ o Äiá»u chuáº©n L2 (`Ridge`) lÃ  **ráº¥t cáº§n thiáº¿t**.
# >
# > * **Táº¡i sao nÃ³ tá»‘t hÆ¡n?** Äiá»u nÃ y kháº³ng Ä‘á»‹nh máº¡nh máº½ giáº£ thuyáº¿t cá»§a chÃºng ta á»Ÿ Pháº§n 5.1: `LinearRegression` gá»‘c Ä‘Ã£ bá»‹ áº£nh hÆ°á»Ÿng náº·ng bá»Ÿi cÃ¡c há»‡ sá»‘ bá»‹ "thá»•i phá»“ng" (do Ä‘a cá»™ng tuyáº¿n vÃ  dá»¯ liá»‡u hiáº¿m nhÆ° `RoofMatl`). GiÃ¡ trá»‹ `alpha=50.0` Ä‘ang "thu nhá»" (shrinking) cÃ¡c há»‡ sá»‘ khÃ´ng Ä‘Ã¡ng tin cáº­y nÃ y, táº¡o ra má»™t mÃ´ hÃ¬nh á»•n Ä‘á»‹nh vÃ  tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.
# >
# > * **`RMSE (log scale) tá»‘t nháº¥t: 0.1446`**: ÄÃ¢y lÃ  Ä‘iá»ƒm lá»—i trung bÃ¬nh (trÃªn 5-fold cross-validation) cá»§a mÃ´ hÃ¬nh `Ridge` Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u. BÃ¢y giá», Ä‘Ã¢y lÃ  "má»‘c" mÃ  `XGBoost` (á»Ÿ pháº§n tiáº¿p theo) cáº§n pháº£i Ä‘Ã¡nh báº¡i.

# %% [markdown]
# #### 5.2.2. Tinh chá»‰nh `XGBoost`
#
# BÃ¢y giá», hÃ£y thá»­ "chá»‘ng há»c váº¹t" (anti-overfit) cho `XGBoost`. ChÃºng ta sáº½ tinh chá»‰nh cÃ¡c tham sá»‘ kiá»ƒm soÃ¡t Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh:
# * `max_depth`: Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a má»—i cÃ¢y. CÃ¢y nÃ´ng hÆ¡n (vÃ­ dá»¥: 3, 4) sáº½ Ã­t bá»‹ há»c váº¹t hÆ¡n.
# * `n_estimators`: Sá»‘ lÆ°á»£ng cÃ¢y.
# * `learning_rate`: Tá»‘c Ä‘á»™ há»c.

# %%
print("\n--- Báº¯t Ä‘áº§u Tinh chá»‰nh XGBoost ---")
print("(BÆ°á»›c nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...)")

# 1. Táº¡o pipeline XGBoost Má»šI
pipe_xgb = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', XGBRegressor(random_state=42, n_jobs=-1))
])

# 2. Äá»‹nh nghÄ©a lÆ°á»›i tham sá»‘ (Param Grid)
# ChÃºng ta sáº½ thá»­ má»™t lÆ°á»›i nhá» Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian
# 3 (max_depth) * 2 (n_estimators) * 2 (learning_rate) = 12 káº¿t há»£p
param_grid_xgb = {
    'model__max_depth': [3, 4, 5],
    'model__n_estimators': [100, 250],
    'model__learning_rate': [0.05, 0.1]
}

# 3. Khá»Ÿi táº¡o GridSearchCV
grid_xgb = GridSearchCV(
    estimator=pipe_xgb,
    param_grid=param_grid_xgb,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1 # In ra tiáº¿n trÃ¬nh
)

# 4. Huáº¥n luyá»‡n (Fit)
# ÄÃ¢y lÃ  bÆ°á»›c tá»‘n thá»i gian nháº¥t
grid_xgb.fit(X_train, y_train)

# 5. In káº¿t quáº£
print("\n--- Káº¿t quáº£ Tinh chá»‰nh XGBoost ---")
print(f"CÃ¡c tham sá»‘ tá»‘t nháº¥t: {grid_xgb.best_params_}")

best_log_rmse_xgb = abs(grid_xgb.best_score_)
print(f"RMSE (trÃªn log scale) tá»‘t nháº¥t tá»« Cross-Validation: {best_log_rmse_xgb:.4f}")

# %% [markdown]
# ### 5.2.3. So sÃ¡nh Cuá»‘i cÃ¹ng (Chung káº¿t)
#
# ChÃºng ta Ä‘Ã£ cÃ³ mÃ´ hÃ¬nh `LinearRegression` gá»‘c, `Ridge` Ä‘Ã£ tinh chá»‰nh, vÃ  `XGBoost` Ä‘Ã£ tinh chá»‰nh.
#
# BÃ¢y giá», hÃ£y cho 3 mÃ´ hÃ¬nh "chung káº¿t" nÃ y dá»± Ä‘oÃ¡n trÃªn táº­p `X_val` (dá»¯ liá»‡u kiá»ƒm tra 20%) vÃ  tÃ­nh toÃ¡n RMSE (báº±ng Ä‘Ã´ la $) láº§n cuá»‘i cÃ¹ng Ä‘á»ƒ xem ai lÃ  ngÆ°á»i chiáº¿n tháº¯ng thá»±c sá»±.

# %%
# 1. Láº¥y ra cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t tá»« GridSearchCV
best_ridge_model = grid_ridge.best_estimator_
best_xgb_model = grid_xgb.best_estimator_

# 2. Láº¥y láº¡i mÃ´ hÃ¬nh LinearRegression gá»‘c
lr_model = pipelines['Linear Regression'] # Tá»« dict 'pipelines' á»Ÿ Pháº§n 4.3

# 3. Táº¡o danh sÃ¡ch cÃ¡c mÃ´ hÃ¬nh chung káº¿t
final_models = {
    "Linear Regression (Gá»‘c)": lr_model,
    "Ridge (ÄÃ£ Tinh chá»‰nh)": best_ridge_model,
    "XGBoost (ÄÃ£ Tinh chá»‰nh)": best_xgb_model
}

# 4. Cháº¡y vÃ²ng láº·p Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng
final_results = []
for name, model in final_models.items():

    # Dá»± Ä‘oÃ¡n trÃªn X_val
    y_pred_log = model.predict(X_val)

    # Äáº£o ngÆ°á»£c vá» $
    y_val_dollar = np.expm1(y_val)
    y_pred_dollar = np.expm1(y_pred_log)

    # TÃ­nh RMSE
    rmse = np.sqrt(mean_squared_error(y_val_dollar, y_pred_dollar))

    # TÃ­nh RÂ²
    r2 = r2_score(y_val_dollar, y_pred_dollar)

    final_results.append({
        "Model": name,
        "Final RMSE ($)": rmse,
        "Final R-squared": r2
    })

# 5. In Báº£ng Chung káº¿t
final_results_df = pd.DataFrame(final_results).sort_values(by='Final RMSE ($)')

print("\n--- Báº¢NG Káº¾T QUáº¢ CHUNG Káº¾T ---")
final_results_df.style.format({
    "Final RMSE ($)": "${:,.2f}",
    "Final R-squared": "{:.4f}"
})

# %% [markdown]
# > **Káº¿t luáº­n (Pháº§n 5):**
# >
# > * (Báº¡n sáº½ Ä‘iá»n káº¿t luáº­n cá»§a mÃ¬nh vÃ o Ä‘Ã¢y sau khi cháº¡y)
# >
# > * **Ai lÃ  ngÆ°á»i chiáº¿n tháº¯ng?** `LinearRegression` cÃ³ giá»¯ Ä‘Æ°á»£c ngÃ´i vÆ°Æ¡ng khÃ´ng? Hay `Ridge` (Ä‘Ã£ tinh chá»‰nh) hoáº·c `XGBoost` (Ä‘Ã£ tinh chá»‰nh) vÆ°á»£t qua nÃ³?
# > * **GiÃ¡ trá»‹ cá»§a Tinh chá»‰nh:** So sÃ¡nh `XGBoost (ÄÃ£ Tinh chá»‰nh)` ($Má»šI) vá»›i `XGBoost (Gá»‘c)` ($26,259). ChÃºng ta Ä‘Ã£ cáº£i thiá»‡n Ä‘Æ°á»£c nÃ³ bao nhiÃªu báº±ng cÃ¡ch chá»‘ng há»c váº¹t?
# > * **MÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘á»ƒ LÆ°u:** Dá»±a trÃªn báº£ng nÃ y, chÃºng ta sáº½ chá»n mÃ´ hÃ¬nh chiáº¿n tháº¯ng cuá»‘i cÃ¹ng Ä‘á»ƒ lÆ°u láº¡i á»Ÿ Pháº§n 7.

# %% [markdown]
# ## 6. ğŸ“ˆ PhÃ¢n tÃ­ch Bá»• sung (Chuá»—i thá»i gian)

# %% [markdown]
# ## 7. ğŸš€ LÆ°u MÃ´ hÃ¬nh
